{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aml lstm model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "QbVnzJ3WIpNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ir-S5B0jF5gk",
        "outputId": "8c94fbe2-1d1c-4972-9dbb-bc707b85e170"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8a09c061-56e4-4038-b6b5-5711946b4269\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>father dysfunct selfish drag kid dysfunction. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>thank #lyft credit can't use caus offer wheelc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday majesti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>#model love u take u time urd+-!!! dddd d|d|d|</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: societi #motiv</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a09c061-56e4-4038-b6b5-5711946b4269')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a09c061-56e4-4038-b6b5-5711946b4269 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a09c061-56e4-4038-b6b5-5711946b4269');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   label                                        clean_tweet\n",
              "0      0  father dysfunct selfish drag kid dysfunction. ...\n",
              "1      0  thank #lyft credit can't use caus offer wheelc...\n",
              "2      0                                     bihday majesti\n",
              "3      0     #model love u take u time urd+-!!! dddd d|d|d|\n",
              "4      0                         factsguide: societi #motiv"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"/content/clean_data.csv\")\n",
        "df = df.iloc[:,[2,4]]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset = [\"clean_tweet\"], inplace=True)"
      ],
      "metadata": {
        "id": "XQ9wzOpiGBD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remover(my_string = \"\"):\n",
        "  values = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ012345678 \")\n",
        "  for item in my_string:\n",
        "    if item not in values:\n",
        "      my_string = my_string.replace(item, \"\")\n",
        "  return my_string\n",
        "df.loc[:,'clean'] = [remover(x) for x in df.clean_tweet]\n",
        "df = df.iloc[:,[0,2]]\n",
        "dff = df.drop(['label'], axis = 1)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(dff, list(df.label), test_size=0.1)"
      ],
      "metadata": {
        "id": "m79xzY1vGSeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=X_temp\n",
        "X['label'] = y_temp\n",
        "nonhate = X[X['label']== 0]\n",
        "hate = X[X.label == 1]\n",
        "nonhatesample = nonhate.sample(n = hate.shape[0])\n",
        "ds = pd.concat([hate, nonhatesample], axis = 0)\n",
        "ds.to_csv(\"/content/dataset_train.csv\")\n",
        "ds_temp = ds\n",
        "testdf = X_test\n",
        "testdf['label'] = y_test\n",
        "ds = pd.concat([ds_temp, testdf], axis = 0)"
      ],
      "metadata": {
        "id": "-skr41O5GaBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_temp, X_test, y_temp, y_test = train_test_split(ds.clean,ds.label, test_size=0.1)\n",
        "X_temp.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc_VwYjMGfex",
        "outputId": "0399b755-65d8-4b6a-db1f-d835bc930ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4594     whitenationalist leader reveal 5 horrifi hope ...\n",
              "6878     abbi nike black write america pornhub click wa...\n",
              "8601     see told you lot christma ball scatter around ...\n",
              "3355             yep  grate everyday  gratitud laurasworld\n",
              "23061    due natur work miss fav themorningfix catchup ...\n",
              "Name: clean, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "UT3EOZxFHfuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "drive.mount('/content/gdrive')\n",
        "path=f'/content/sentiment_analysis.pt'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9M-e89DHiDZ",
        "outputId": "a940ffe8-f33d-4a71-dd57-6508c0aaac71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLUt2z51Hi4L",
        "outputId": "930aafa7-31f9-4701-a0ed-ea03ced256ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(s):\n",
        "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    s = re.sub(r\"\\s+\", '', s)\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "    \n",
        "    return s"
      ],
      "metadata": {
        "id": "1FRqRcl_3cTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tockenize(x_train,x_val,x_test):\n",
        "    word_list = []\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    for sentence in x_train:\n",
        "        for word in sentence.lower().split():\n",
        "            word = preprocess(word)\n",
        "            \n",
        "            if word not in stop_words and word != '':\n",
        "                lemmatizer = WordNetLemmatizer()\n",
        "                wordnet_lemmatizer=lemmatizer.lemmatize(word)\n",
        "\n",
        "                word_list.append(wordnet_lemmatizer)\n",
        "\n",
        "    Count = Counter(word_list)\n",
        "    \n",
        "    corpus = sorted(Count,key=Count.get,reverse=True)\n",
        "\n",
        "    freq_words = {w:i+1 for i,w in enumerate(corpus)}\n",
        "    \n",
        "    train_set,test_set,valid_set = [],[],[]\n",
        "    for i in x_train:\n",
        "            train_set.append([freq_words[preprocess(word)] for word in i.lower().split() \n",
        "                                     if preprocess(word) in freq_words.keys()])\n",
        "    for i in x_val:\n",
        "            valid_set.append([freq_words[preprocess(word)] for word in i.lower().split() \n",
        "                                    if preprocess(word) in freq_words.keys()])\n",
        "    \n",
        "    for i in x_test:\n",
        "            test_set.append([freq_words[preprocess(word)] for word in i.lower().split()\n",
        "                                  if preprocess(word) in freq_words.keys()])\n",
        "    \n",
        "    return np.array(train_set),np.array(test_set),np.array(valid_set),freq_words\n"
      ],
      "metadata": {
        "id": "xHQpXFXkHmcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_val,x_test,vocab = tockenize(X_temp,X_test,X_test)\n"
      ],
      "metadata": {
        "id": "UBLHn8BCIoYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df26ef0-b0f2-4443-fbe4-50c7db667ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjACSzteMfPF",
        "outputId": "86373278-15a4-4a86-b8df-d37df4ecc6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[list([2256, 706, 767, 1774, 133, 94])\n",
            " list([4587, 3008, 19, 604, 94, 4588, 1497, 81])\n",
            " list([20, 558, 243, 443, 848, 3009, 179, 4589]) ...\n",
            " list([1127, 13181, 197, 1576, 1127, 1576, 2839, 1965, 750, 3456])\n",
            " list([53, 263, 178, 9004, 840, 713, 37])\n",
            " list([42, 175, 1259, 550, 24, 456])]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6498,)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def padding(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ],
      "metadata": {
        "id": "Z0mkWlviI0OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pad = padding(x_train,500)\n",
        "x_valid_pad = padding(x_test,500)\n"
      ],
      "metadata": {
        "id": "GlVtISqBJIZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3nDFvQFLNte",
        "outputId": "5a6c1377-f487-4ffd-dcbd-920e36987b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6498,)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y=np.array(y_temp)\n",
        "valid_y=np.array(y_test)"
      ],
      "metadata": {
        "id": "Ar9XsxfJKplg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm9eM6EUOV1j",
        "outputId": "2b88ab3a-336c-4b8d-d417-846c53d93917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(722,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(x_valid_pad), torch.from_numpy(valid_y))\n",
        "# training and validation dataloaders\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "metadata": {
        "id": "m7d-p_x8Krq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()"
      ],
      "metadata": {
        "id": "cB3F2Md_LJ8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
        "        super(LSTM,self).__init__()\n",
        " \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        " \n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "    \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                           num_layers=no_layers, batch_first=True)\n",
        "        \n",
        "        \n",
        "        # dropout layer/ regularization\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "    \n",
        "        # linear and sigmoid layer\n",
        "        self.lin = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "      \n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        embeds = self.embedding(x)  \n",
        "        \n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.lin(out)\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        sig_out = sig_out[:, -1]\n",
        " \n",
        "        return sig_out, hidden\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "j2LZychhOwjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1 \n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 128\n",
        "model = LSTM(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yap4b70NOyM1",
        "outputId": "37ddb2d6-80b2-4d7d-a208-e1d4fb7a2150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (embedding): Embedding(13182, 64)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            "  (lin): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()"
      ],
      "metadata": {
        "id": "itAKWoumOz-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(batch_size,train_loader,valid_loader,):\n",
        "  clip = 5\n",
        "  epochs = 20\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "  epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      train_losses = []\n",
        "      train_acc = 0.0\n",
        "      model.train()\n",
        " \n",
        "      \n",
        "      h = model.init_hidden(batch_size)\n",
        "  \n",
        "      for inputs, labels in train_loader:\n",
        "          \n",
        "          inputs, labels = inputs.to(device), labels.to(device)   \n",
        "          h = tuple([each.data for each in h])\n",
        "          \n",
        "          model.zero_grad()\n",
        "          output,h = model(inputs,h)\n",
        "          loss = criterion(output.squeeze(), labels.float())\n",
        "          loss.backward()\n",
        "          train_losses.append(loss.item())\n",
        "          accuracy = acc(output,labels)\n",
        "          train_acc += accuracy\n",
        "          \n",
        "         \n",
        "          nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          optimizer.step()\n",
        "  \n",
        "      \n",
        "        \n",
        "      val_h = model.init_hidden(batch_size)\n",
        "      val_losses = []\n",
        "\n",
        "      val_acc = 0.0\n",
        "      model.eval()\n",
        "\n",
        "      for inputs, labels in valid_loader:\n",
        "              val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              output, val_h = model(inputs, val_h)\n",
        "              val_loss = criterion(output.squeeze(), labels.float())\n",
        "              \n",
        "              val_losses.append(val_loss.item())\n",
        "              \n",
        "              accuracy = acc(output,labels)\n",
        "              val_acc += accuracy\n",
        "\n",
        "\n",
        "              \n",
        "      epoch_train_loss = np.mean(train_losses)\n",
        "      epoch_val_loss = np.mean(val_losses)\n",
        "      \n",
        "      epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "      epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "      \n",
        "      epoch_tr_loss.append(epoch_train_loss)\n",
        "      epoch_vl_loss.append(epoch_val_loss)\n",
        "      \n",
        "      epoch_tr_acc.append(epoch_train_acc)\n",
        "      epoch_vl_acc.append(epoch_val_acc)\n",
        "      \n",
        "      print(f'Epoch {epoch+1}') \n",
        "      print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss} ')\n",
        "      print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "      if epoch_val_loss <= valid_loss_min:\n",
        "          torch.save(model.state_dict(), path)\n",
        "          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "          valid_loss_min = epoch_val_loss\n",
        "      print(50*'==')\n",
        "  return model\n",
        "training(batch_size,train_loader,valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRhNqvHlO1yJ",
        "outputId": "d979214c-3991-418a-c611-57e1a090644a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "train_loss : 0.5506239477978196 val_loss : 0.45393866300582886 \n",
            "train_accuracy : 71.9298245614035 val_accuracy : 77.42382271468145\n",
            "Validation loss decreased (inf --> 0.453939).  Saving model ...\n",
            "====================================================================================================\n",
            "Epoch 2\n",
            "train_loss : 0.3843205996262011 val_loss : 0.36231583995478495 \n",
            "train_accuracy : 82.40997229916897 val_accuracy : 81.16343490304709\n",
            "Validation loss decreased (0.453939 --> 0.362316).  Saving model ...\n",
            "====================================================================================================\n",
            "Epoch 3\n",
            "train_loss : 0.2706322820149651 val_loss : 0.3528894292456763 \n",
            "train_accuracy : 88.47337642351492 val_accuracy : 83.37950138504155\n",
            "Validation loss decreased (0.362316 --> 0.352889).  Saving model ...\n",
            "====================================================================================================\n",
            "Epoch 4\n",
            "train_loss : 0.1828302895837976 val_loss : 0.3645839233483587 \n",
            "train_accuracy : 92.19759926131117 val_accuracy : 83.10249307479224\n",
            "====================================================================================================\n",
            "Epoch 5\n",
            "train_loss : 0.1151748696529819 val_loss : 0.40578421843903406 \n",
            "train_accuracy : 95.42936288088643 val_accuracy : 83.65650969529086\n",
            "====================================================================================================\n",
            "Epoch 6\n",
            "train_loss : 0.06820989741633336 val_loss : 0.45488001831940245 \n",
            "train_accuracy : 97.16835949522931 val_accuracy : 83.5180055401662\n",
            "====================================================================================================\n",
            "Epoch 7\n",
            "train_loss : 0.05139115355482917 val_loss : 0.4800946499620165 \n",
            "train_accuracy : 97.72237611572791 val_accuracy : 83.10249307479224\n",
            "====================================================================================================\n",
            "Epoch 8\n",
            "train_loss : 0.02876830551332917 val_loss : 0.598948033792632 \n",
            "train_accuracy : 98.41489689135119 val_accuracy : 83.65650969529086\n",
            "====================================================================================================\n",
            "Epoch 9\n",
            "train_loss : 0.014861177964417566 val_loss : 0.6419334166816303 \n",
            "train_accuracy : 98.89196675900277 val_accuracy : 83.79501385041551\n",
            "====================================================================================================\n",
            "Epoch 10\n",
            "train_loss : 0.0090167146278076 val_loss : 0.6855877020529338 \n",
            "train_accuracy : 99.06124961526623 val_accuracy : 83.65650969529086\n",
            "====================================================================================================\n",
            "Epoch 11\n",
            "train_loss : 0.004970965073006263 val_loss : 0.7518606441361564 \n",
            "train_accuracy : 99.15358571868266 val_accuracy : 84.34903047091413\n",
            "====================================================================================================\n",
            "Epoch 12\n",
            "train_loss : 0.002516505663645726 val_loss : 0.8468738773039409 \n",
            "train_accuracy : 99.21514312096029 val_accuracy : 84.21052631578947\n",
            "====================================================================================================\n",
            "Epoch 13\n",
            "train_loss : 0.002047809397805931 val_loss : 0.8551905070032392 \n",
            "train_accuracy : 99.21514312096029 val_accuracy : 83.37950138504155\n",
            "====================================================================================================\n",
            "Epoch 14\n",
            "train_loss : 0.0027178197454396265 val_loss : 0.8813625744410923 \n",
            "train_accuracy : 99.15358571868266 val_accuracy : 82.82548476454294\n",
            "====================================================================================================\n",
            "Epoch 15\n",
            "train_loss : 0.0024722382510753253 val_loss : 0.9527878484555653 \n",
            "train_accuracy : 99.18436441982148 val_accuracy : 82.54847645429363\n",
            "====================================================================================================\n",
            "Epoch 16\n",
            "train_loss : 0.0019618484361143243 val_loss : 0.8936445947204318 \n",
            "train_accuracy : 99.19975377039088 val_accuracy : 83.2409972299169\n",
            "====================================================================================================\n",
            "Epoch 17\n",
            "train_loss : 0.0021713222984883116 val_loss : 0.9178079153810229 \n",
            "train_accuracy : 99.16897506925207 val_accuracy : 82.82548476454294\n",
            "====================================================================================================\n",
            "Epoch 18\n",
            "train_loss : 0.001379071441353813 val_loss : 0.9822951682976314 \n",
            "train_accuracy : 99.19975377039088 val_accuracy : 82.96398891966759\n",
            "====================================================================================================\n",
            "Epoch 19\n",
            "train_loss : 0.0014763954191603403 val_loss : 0.9461313273225512 \n",
            "train_accuracy : 99.18436441982148 val_accuracy : 83.10249307479224\n",
            "====================================================================================================\n",
            "Epoch 20\n",
            "train_loss : 0.001362203532526885 val_loss : 0.9685736415641648 \n",
            "train_accuracy : 99.18436441982148 val_accuracy : 83.37950138504155\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(13182, 64)\n",
              "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True)\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              "  (lin): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (sig): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(text):\n",
        "        word_seq = np.array([vocab[preprocess(word)] for word in text.split() \n",
        "                         if preprocess(word) in vocab.keys()])\n",
        "        word_seq = np.expand_dims(word_seq,axis=0)\n",
        "        pad =  torch.from_numpy(padding(word_seq,500))\n",
        "        inputs = pad.to(device)\n",
        "        batch_size = 1\n",
        "        h = model.init_hidden(batch_size)\n",
        "        h = tuple([each.data for each in h])\n",
        "        output, h = model(inputs, h)\n",
        "        return(output)"
      ],
      "metadata": {
        "id": "8TpJErZKO4IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classification_metric(testy, probs):\n",
        "    from sklearn.metrics import precision_recall_curve\n",
        "    precision, recall, thresholds = precision_recall_curve(testy, probs)\n",
        "    # convert to f score\n",
        "    fscore = (2 * precision * recall) / (precision + recall)\n",
        "    # locate the index of the largest f score\n",
        "    ix = np.argmax(fscore)\n",
        "    return fscore[ix]"
      ],
      "metadata": {
        "id": "q04wuPlPO8Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label).item()"
      ],
      "metadata": {
        "id": "E7zc1wVMVM7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "pre_acc=0.0\n",
        "pred=[]\n",
        "test_x=X_test\n",
        "test_y=y_test\n",
        "print(len(test_x),len(test_y))\n",
        "for i in test_x:\n",
        "  prediction = predict_text(i)\n",
        "  status = \"negative\" if prediction > 0.5 else \"positive\"\n",
        "  prediction= 0 if status==\"positive\" else 1\n",
        "  pred.append(prediction)\n",
        "  \n",
        "  #print(test_y[index],pre_acc,prediction,status)\n",
        "  index+=1\n",
        "  \n",
        "\n",
        "f1=f1_score(test_y,pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLK8MpBUPA5i",
        "outputId": "6883077f-720e-4440-b604-84519c8feffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "722 722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"=\"*50)\n",
        "print(\"acc\",accuracy_score(y_test, pred)*100)\n",
        "print(\"F1 Score:\",f1*100)\n",
        "print(\"=\"*50)\n",
        "print(\"roc\",roc_auc_score(test_y, pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz36qse-PDYT",
        "outputId": "3cea85ac-f5df-4ad9-8ad8-c55149e62a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "acc 85.87257617728532\n",
            "F1 Score: 76.49769585253456\n",
            "==================================================\n",
            "roc 83.19934297577223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hNxNb__WXvtr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}