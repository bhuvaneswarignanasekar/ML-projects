{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T5duTylnDrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a0e075-7fcd-4d13-a2bf-0a1b80304abf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emy5QBh5qfKq",
        "outputId": "07a31155-72ad-417d-a81e-3407c768cda0"
      },
      "source": [
        "!pip install --quiet transformers==4.5.0\n",
        "#!pip install --quiet pytorch-lightenining==1.2.7\n",
        "!pip install torchtext==0.8.0 torch==1.7.1 pytorch-lightning==1.2.2\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.0 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.8.0\n",
            "  Downloading torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |██                              | 48.0 MB 1.9 MB/s eta 0:06:17"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_uR6tgqj8r"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from termcolor import colored\n",
        "import textwrap\n",
        "import datasets\n",
        "from transformers import(\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast as T5Tokenizer\n",
        ")\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIaYiaRMqmls"
      },
      "source": [
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_formats='retina'\n",
        "sns.set(style='whitegrid',palette='muted',font_scale=1.2)\n",
        "rcParams['figure.figsize']=16,10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxs-yedlqoss"
      },
      "source": [
        "\n",
        "pl.seed_everything(42)\n",
        "train_data = datasets.load_dataset(\"xsum\", split=\"train\")\n",
        "val_data = datasets.load_dataset(\"xsum\", split=\"validation\")\n",
        "test_data = datasets.load_dataset(\"xsum\", split=\"test\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZJd_dbT4DCY"
      },
      "source": [
        "pl.seed_everything(42)\n",
        "train_data=pd.read_csv('/content/train_data.csv',encoding=\"latin-1\",engine='python', error_bad_lines=False)\n",
        "val_data=pd.read_csv('/content/val_data.csv',encoding=\"latin-1\",engine='python', error_bad_lines=False)\n",
        "test_data=pd.read_csv('/content/val_data.csv',encoding=\"latin-1\",engine='python', error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toi9F_AG4-HJ"
      },
      "source": [
        "train_data=train_data[[\"document\",\"summary\"]]\n",
        "test_data=test_data[[\"document\",\"summary\"]]\n",
        "val_data=val_data[[\"document\",\"summary\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVegktKZnrEI"
      },
      "source": [
        "print('full: ',train_data.iloc[0, 0], '\\n')\n",
        "print('summary:', train_data.iloc[0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDRR6ZG68GCd"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTiAaSxUtyv3"
      },
      "source": [
        "class NewSummaryDataset(Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      data,\n",
        "      tokenizer:T5Tokenizer,\n",
        "      text_max_token_len: int= 512,\n",
        "      summary_max_token_len=128\n",
        "  ):\n",
        "    self.tokenizer=tokenizer\n",
        "    self.data=data\n",
        "    self.text_max_token_len=text_max_token_len\n",
        "    self.summary_max_token_len=summary_max_token_len\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    \n",
        "  def __getitem__(self,index: int):\n",
        "    data_row=self.data.iloc[index]\n",
        "    text=data_row[\"document\"]\n",
        "    text_encoding=tokenizer(\n",
        "        text,\n",
        "        max_length=self.text_max_token_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    summary_encoding=tokenizer(\n",
        "        text,\n",
        "        max_length=self.summary_max_token_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels=summary_encoding[\"input_ids\"]\n",
        "    labels[labels==0]=-100\n",
        "\n",
        "    return dict(\n",
        "        text=text,\n",
        "        summary=data_row[\"summary\"],\n",
        "        text_input_ids=text_encoding[\"input_ids\"].flatten(),\n",
        "        text_attention_mask=text_encoding[\"attention_mask\"].flatten(),\n",
        "        labels=labels.flatten(),\n",
        "        labels_attention_mask=summary_encoding[\"attention_mask\"].flatten()\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElQhTevZv8BB"
      },
      "source": [
        "class NewSummaryDataModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      train_data,\n",
        "      test_data,\n",
        "      tokenizer:T5Tokenizer,\n",
        "      text_max_token_len: int= 512,\n",
        "      summary_max_token_len=128\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.train_df=train_data\n",
        "    self.test_df=test_data\n",
        "\n",
        "    self.batch_size= batch_size\n",
        "    self.tokenizer=tokenizer\n",
        "    self.text_max_token_len=text_max_token_len\n",
        "    self.summary_max_token_len=summary_max_token_len\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    self.train_dataset=NewSummaryDataset(\n",
        "        self.train_df,\n",
        "        self.tokenizer,\n",
        "        self.text_max_token_len,\n",
        "        self.summary_max_token_len\n",
        "    )\n",
        "    self.test_dataset=NewSummaryDataset(\n",
        "        self.test_df,\n",
        "        self.tokenizer,\n",
        "        self.text_max_token_len,\n",
        "        self.summary_max_token_len\n",
        "    )\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkt8DKaryRm0"
      },
      "source": [
        "Model_name=\"t5-base\"\n",
        "tokenizer=T5Tokenizer.from_pretrained(Model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEW6xjeY9SCq"
      },
      "source": [
        "\"\"\"text_token_counts,summary_token_counts=[],[]\n",
        "for i, row in train_dataloadertext\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqrGj0uiyfFE"
      },
      "source": [
        "epoch=4\n",
        "batch_size=10\n",
        "data_module=NewSummaryDataModule(train_data,test_data,tokenizer,batch_size)\n",
        "print(data_module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3uE798fy7GB"
      },
      "source": [
        "class NewSummaryModule(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model=T5ForConditionalGeneration.from_pretrained(Model_name,return_dict=True)\n",
        "  def forward(self, input_ids, attention_mask,decoder_attention_mask,labels=None):\n",
        "    output= self.model(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        labels=labels,\n",
        "        decoder_attention_mask=decoder_attention_mask\n",
        "    )\n",
        "    return output.loss, output.logits\n",
        "\n",
        "  def training_step(self,batch,batch_idx):\n",
        "    input_ids=batch[\"text_input_ids\"]\n",
        "    attention_mask=batch[\"text_attention_mask\"]\n",
        "    labels=batch[\"labels\"]\n",
        "    labels_attention_mask=batch[\"labels_attention_mask\"]\n",
        "\n",
        "    loss,outputs=self(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        decoder_attention_mask=labels_attention_mask,\n",
        "        labels=labels\n",
        "    )    \n",
        "    self.log(\"train_loss\", loss,prog_bar=True,logger=True)\n",
        "    return loss\n",
        "  def val_step(self,batch,batch_idx):\n",
        "    input_ids=batch[\"text_input_ids\"]\n",
        "    attention_mask=batch[\"text_attention_mask\"]\n",
        "    labels=batch[\"labels\"]\n",
        "    labels_attention_mask=batch[\"labels_attention_mask\"]\n",
        "\n",
        "    loss,outputs=self(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        decoder_attention_mask=labels_attention_mask,\n",
        "        labels=labels\n",
        "    )    \n",
        "    self.log(\"val_loss\", loss,prog_bar=True,logger=True)\n",
        "    return loss\n",
        "  def test_step(self,batch,batch_idx):\n",
        "    input_ids=batch[\"text_input_ids\"]\n",
        "    attention_mask=batch[\"text_attention_mask\"]\n",
        "    labels=batch[\"labels\"]\n",
        "    labels_attention_mask=batch[\"labels_attention_mask\"]\n",
        "\n",
        "    loss,outputs=self(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        decoder_attention_mask=labels_attention_mask,\n",
        "        labels=labels\n",
        "    )    \n",
        "    self.log(\"test_loss\", loss,prog_bar=True,logger=True)\n",
        "    return loss\n",
        "  def configure_optimizers(self):\n",
        "    return AdamW(self.parameters(),lr=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXTisI51GUC"
      },
      "source": [
        "model=NewSummaryModule()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQcOdfc1YFV"
      },
      "source": [
        "#%load_ext tensorboard \n",
        "#%tensorboard --logdir ./lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZVsMkvx20Pu"
      },
      "source": [
        "checkpoint_callback=ModelCheckpoint(\n",
        "    dirpath=\"checkpoints\",\n",
        "    filename=\"best_checkpoint\",\n",
        "    save_last=True,\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "logger=TensorBoardLogger(\"lightning_logs\",name=\"news-summary\")\n",
        "\n",
        "trainer=pl.Trainer(\n",
        "    logger=logger,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    max_epochs=epoch,\n",
        "    gpus=1,\n",
        "    progress_bar_refresh_rate=30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pQ5OKZ-3p7S"
      },
      "source": [
        "trainer.fit(model,data_module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JE-fCjW3udE"
      },
      "source": [
        "def predict_text(text):\n",
        "  preprocess_function(text)\n",
        "  generated_ids=model.generate(\n",
        "           input_ids=text_encoding[\"inputs_ids\"],\n",
        "           attention_mask=text_encoding[\"attention_mask\"],\n",
        "           max_length=150,\n",
        "           num_beams=2,\n",
        "           repitition_penalty=2.5,\n",
        "           length_penalty=1.0,\n",
        "           early_stopping=True)\n",
        "  preds=[tokenizer.decode(gen_id,skip_special_tokens=True,clean_up_tokenization_spaces=True) for gen_id in generated_ids]\n",
        "  return \"\".join(preds)\n",
        "for i in raw_datasets[\"test\"][:2]:\n",
        "  predict_text(i['document'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}